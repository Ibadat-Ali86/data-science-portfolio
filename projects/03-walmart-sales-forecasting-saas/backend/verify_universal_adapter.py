import requests
import pandas as pd
import io
import json
import time

BASE_URL = "http://localhost:8000/api"

def create_mock_csv(domain):
    """Create in-memory CSV for testing"""
    if domain == 'sales':
        df = pd.DataFrame({
            'Date': pd.date_range(start='2023-01-01', periods=100),
            'Sales': [x * 10 + 50 for x in range(100)],
            'Item_ID': ['ITEM_001'] * 100,
            'Store_ID': ['STORE_A'] * 100
        })
    elif domain == 'hr':
        df = pd.DataFrame({
            'Employee_ID': [f'EMP_{i}' for i in range(50)],
            'Hire_Date': pd.date_range(start='2020-01-01', periods=50),
            'Status': ['Active'] * 45 + ['Terminated'] * 5,
            'Salary': [50000 + x * 1000 for x in range(50)]
        })
    elif domain == 'finance':
        df = pd.DataFrame({
            'Posting_Date': pd.date_range(start='2023-01-01', periods=100),
            'Amount': [1000.50] * 100,
            'Account_ID': ['ACC_123'] * 100,
            'Description': ['Service Fee'] * 100
        })
    else:
        return None
        
    csv_buffer = io.StringIO()
    df.to_csv(csv_buffer, index=False)
    return csv_buffer.getvalue()

def test_domain_pipeline(domain_name):
    print(f"\n--- Testing Domain: {domain_name.upper()} ---")
    
    # 1. Upload & Detect
    csv_content = create_mock_csv(domain_name)
    files = {'file': ('test.csv', csv_content, 'text/csv')}
    
    # Step A: /detect-format (Schema Detection)
    print("Step 1: Detecting Format...")
    try:
        # We need to use data_pipeline.detect_format logic
        # But wait, the frontend calls /detect-format with the file
        # Let's check the endpoint path in backend/app/api/data_pipeline.py
        # It is router.post("/detect-format")
        # Full URL: http://localhost:8000/api/data-pipeline/detect-format
        
        response = requests.post(f"{BASE_URL}/data-pipeline/detect-format", files=files)
        if response.status_code != 200:
            print(f"‚ùå Detect Format Failed: {response.text}")
            return False
            
        result = response.json()
        print(f"‚úÖ Detection Result: {result.get('columns')}")
        
        schema_analysis = result.get('schema_analysis')
        if not schema_analysis:
            print("‚ùå No Schema Analysis returned!")
            return False
            
        print(f"‚úÖ Detected Domain: {schema_analysis.get('domain')} (Confidence: {schema_analysis.get('confidence')})")
        
        expected_key = {
            'sales': 'sales_forecast',
            'hr': 'hr_analytics',
            'finance': 'financial_metrics'
        }[domain_name]
        
        # Check domain key (if we exposed it, usually it's internal but let's check domain name or derived key)
        # We added domain_key to response!
        detected_key = schema_analysis.get('domain_key')
        if detected_key != expected_key:
             print(f"‚ö†Ô∏è Warning: Domain Mismatch. Expected {expected_key}, got {detected_key}")
             
    except Exception as e:
        print(f"‚ùå Exception in detection: {e}")
        return False

    # Step B: Upload (Session Creation)
    # We need to re-upload to /analysis/upload to get a session_id
    # Note: files pointer is exhausted, recreate
    files = {'file': ('test.csv', csv_content, 'text/csv')} # Helper function exhausted buffer? No, string content is fine.
    
    print("Step 2: Uploading for Analysis...")
    upload_resp = requests.post(f"{BASE_URL}/analysis/upload", files=files)
    if upload_resp.status_code != 200:
        print(f"‚ùå Upload Failed: {upload_resp.text}")
        return False
        
    session_data = upload_resp.json()
    session_id = session_data.get('session_id')
    print(f"‚úÖ Session Created: {session_id}")
    
    # Step C: Profile (KPIs & Narrative)
    print("Step 3: Profiling Dataset...")
    profile_req = {
        "target_col": result['suggested_mapping'].get('target', 'Sales'), # Heuristic
        "date_col": result['suggested_mapping'].get('date', 'Date')
    }
    
    # Need to handle mapping differences manually for test
    if domain_name == 'hr':
         profile_req = {"target_col": "Salary", "date_col": "Hire_Date"}
    elif domain_name == 'finance':
         profile_req = {"target_col": "Amount", "date_col": "Posting_Date"}
         
    profile_resp = requests.post(f"{BASE_URL}/analysis/profile/{session_id}", json=profile_req)
    if profile_resp.status_code != 200:
         print(f"‚ùå Profile Failed: {profile_resp.text}")
         return False
         
    profile = profile_resp.json()
    
    # Verify Dynamic KPIs
    kpis = profile.get('dynamic_kpis', [])
    print(f"‚úÖ Generated {len(kpis)} Dynamic KPIs")
    for kpi in kpis:
        print(f"   - {kpi['name']}: {kpi['value']} {kpi.get('suffix', '')}")
        
    # Verify Narrative
    narrative = profile.get('narrative_report')
    if narrative:
        print(f"‚úÖ Narrative Report Generated: {narrative['title']}")
        print(f"   - Sections: {[s['heading'] for s in narrative['sections']]}")
    else:
        print("‚ùå No Narrative Report found")
        
    return True

if __name__ == "__main__":
    print("üöÄ Starting Universal Adapter Verification...")
    
    success_sales = test_domain_pipeline('sales')
    success_hr = test_domain_pipeline('hr')
    # success_finance = test_domain_pipeline('finance') # Optional
    
    if success_sales and success_hr:
        print("\n‚úÖ‚úÖ VERIFICATION SUCCESSFUL: System correctly handles multiple data domains! ‚úÖ‚úÖ")
    else:
        print("\n‚ùå VERIFICATION FAILED")
